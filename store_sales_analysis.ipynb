# step:1.libraries import
importn numby as np
import pandas as pd
importb matplotlib.pyplot as plt
from sklearn.preprocessing import MinMaxScaller
from sklearn.metrics import mean_squared_error
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM,Dense
import shap

# step:2. Load Dataset
data=pd.read_csv("store_sales.csv")
print(data.head())

# step:3.Basic EDA
print("Shape:",data.shape)
print(data.info())
print.(describe())

# step:4. Handle Missing Values
print(data.isnull().sum())
data['sales']=
data['sales'].fillna(method='ffill')
data['promo']=data['promo'].fillna(0)
data['holiday']=data['holiday'].fillna(0)

#step:5.Date formatting & sorting
data['date']=pd.to_datetime(data['date'])
data=data.sort_values('date')
store1=data[data['store']==1]
store1.set_index('date',inplace=True)

#step:6.Plot (Visualization)
plt.figure(figsize=10,5))
plt.plot(store1.index,store1['sales'])
plt.title("Sales over time")
plt.xlabel("Date")
plt.ylabel("Sales")
plt.show()

plt.figure(figsize=(10,5))
plt.plot(store1['sales'],label="Sales")
plt.plot(store1['promo'],label="Promo")
plt.plot(store1['holiday'],label="Holiday")
plt.legend()
plt.title("Sales vs Promo vs Holiday")
plt.show()


# step:7.Features Selection &Scaling
features=store1[['sales','promo','holiday']]
scaler=MinMaxScaler()
scaled_data=scaler.fit_transform(features)

#step:8.Create Time Serious Sequences
def create_dataset(dataset,look_back=10):
X,y=[],[]
for i in range(len(dataset-lool_back):
X.append(dataset[i:i+look_back,:])
y.append(dataset[i+look_back,0]) # sales only
return np.array(X),np.array(y)
look_back=10
X,y=create_dataset(scaled_data,look_back)

#step:9.Train_test split(Time Based)
train_size=int(len(X)*0.8
X_train,X_test=X[:train_size],X[train_size:]
y_train,y_test=y[:train_size],y[train_size:]

#step:10. Build LSTM model
model=Sequential()
model.add(LSTM(50,input_shape=(look_back,X.shape[2])))
model.add(Dense(1))
model.compile(optimizer='adam',loss='mse')
model.summary()

# step:11. Train Model
history=model.fit(X_train,y_train,
epochs=20,
batch_size=16,
validation_data=(X-test,y_test))

#step:12.Prediction
train_pred=model.predict(X_train)
test_pred=model.predict(X_test)
#inverse scaling (sales only)
sales_scaler=MinMaxScaler()
sales_scaler.fit(store1[['sales']])
train_pred_inv=sales_scaler.inverse_transform(train_pred)
y_train_pred_inv=sales_scaler.inverse_transform(y_train.reshape(-1,1))
test_pred_inv=sales_scaler.inverse_transform(test_pred)
y_test_pred_inv=sales_scaler.inverse_transform(y_test.reshape(-1,1))

#step:13.Evaluation(RMSE)
train_rmse=
np.sqrt(mean_squared_error(y_train_inv,train_pred_inv))
test_rmse=np.sqrt(mean_squared_error(y_test_inv,test_pred_inv))

print("Train RMSE:",train_rmse)
print("Test RMSE:",test_rmse)

#step:14.Plot predictions
plt.figure(figsize=(10,5))
plt.plot(store1.index,store1['sales'],label="Actual")

train_plot=np.empty_like(store1['sales'])
train_plot[:]=np.nan
train_plot[look_back:train_size+look_back]=train_pred_inv.flatten()

test_plot=np.empty_like(store1['sales']
test_plot[:]=np.nan
test_plot[train_size+look_back:]=test_pred_inv.flatten()

plt.plot(store1.index,train_plot,label="Train Prediction")
plt.plot(store1.index,test_plot,label="Test Prediction")
plt.legend()
plt.title("Actual vs Predicted Sales")
plt.show()

#step:15. Explainability using SHAP
explainer=shap.DeepExplainer(model,X_train[:100])
shap_values=explainer.shap_values(X_test[:10])
shap.summary_plot(shap_values[0],X_test[:10])
